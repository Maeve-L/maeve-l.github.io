<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Estimation Prior and Posterior Distributions  Suppose that one has a statistical model with parameter $\theta$. If one treats $\theta$ as random, then the distribution that one assigns to $\theta$ before observing the other random variables of interest is called its prior distribution."><title>Statistical Inference</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://maeve-l.github.io//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&display=swap" rel=stylesheet><link href=https://maeve-l.github.io/styles.cd61336c89ed6e03702366ce4a492b75.min.css rel=stylesheet><script src=https://maeve-l.github.io/js/darkmode.46b07878b7f5d9e26ad7a3c40f8a0605.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><script>const BASE_URL="https://maeve-l.github.io/",fetchData=Promise.all([fetch("https://maeve-l.github.io/indices/linkIndex.1b95f3b1eea826f12704fd0cc513dc4a.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://maeve-l.github.io/indices/contentIndex.9895d9a4b8876c2e1f1e339cc01a0b95.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n}))</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://maeve-l.github.io/js/search.bc849b857f2c1b822264d40635bb67b6.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://maeve-l.github.io/>Quant Apprentice</a></h1><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Statistical Inference</h1><p class=meta>Last updated April 24, 2022</p><ul class=tags></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#estimation>Estimation</a><ol><li><a href=#prior-and-posterior-distributions>Prior and Posterior Distributions</a></li><li><a href=#conjugate-prior-distributions>Conjugate Prior Distributions</a></li><li><a href=#bayes-estimators>Bayes Estimators</a></li><li><a href=#maximum-likelihood-estimators>Maximum Likelihood Estimators</a></li><li><a href=#properties-of-maximum-likelihood-estimators>Properties of Maximum Likelihood Estimators</a></li></ol></li><li><a href=#sampling-distributions-of-estimators>Sampling Distributions of Estimators</a><ol><li><a href=#joint-distribution-of-the-sample-mean-and-sample-variance>Joint Distribution of the Sample Mean and Sample Variance</a></li><li><a href=#confidence-intervals>Confidence Intervals</a></li><li><a href=#unbiased-estimators>Unbiased Estimators</a></li></ol></li><li><a href=#testing-hypotheses>Testing Hypotheses</a><ol><li><a href=#problems-of-testing-hypotheses>Problems of Testing Hypotheses</a></li><li><a href=#the-t-test>The t Test</a></li><li><a href=#comparing-the-means-of-two-normal-distributions>Comparing the Means of Two Normal Distributions</a></li><li><a href=#the-f-distributions>The F Distributions</a></li></ol></li><li><a href=#categorical-data-and-nonparametric-methods>Categorical Data and Nonparametric Methods</a><ol><li><a href=#tests-of-goodness-of-fit>Tests of Goodness-of-Fit</a></li><li><a href=#goodness-of-fit-for-composite-hypotheses>Goodness-of-Fit for Composite Hypotheses</a></li><li><a href=#contingency-tables>Contingency Tables</a></li><li><a href=#tests-of-homogeneity>Tests of Homogeneity</a></li><li><a href=#simpsons-paradox>Simpson’s Paradox</a></li></ol></li><li><a href=#four-steps-to-hypothesis-testing>Four Steps to Hypothesis Testing</a><ol><li><a href=#compute-the-test-statistic>Compute the test statistic</a></li></ol></li></ol></nav></details></aside><h2 id=estimation>Estimation</h2><h3 id=prior-and-posterior-distributions>Prior and Posterior Distributions</h3><ul><li>Suppose that one has a statistical model with parameter $\theta$. If one treats $\theta$ as random, then the distribution that one assigns to $\theta$ before observing the other random variables of interest is called its prior distribution.</li><li>Consider a statistical inference problem with parameter $\theta$ and random variables $X_1, . . . , X_n$ to be observed. The conditional distribution of $\theta$ given $X_1, . . . , X_n$ is called the posterior distribution of $\theta$.</li><li>Suppose that the $n$ random variables $X_1, . . . , X_n$ form a random sample from a distribution for which the p.d.f. or the p.f. is $f (x|\theta)$. Suppose also that the value of the parameter $\theta$ is unknown and the prior p.d.f. or p.f. of $\theta$ is $\xi(\theta)$. Then the posterior p.d.f. or p.f. of $\theta$ is</li></ul><p>$$\xi(\theta \mid \boldsymbol{x})=\frac{f\left(x_{1} \mid \theta\right) \cdots f\left(x_{n} \mid \theta\right) \xi(\theta)}{g_{n}(\boldsymbol{x})} \quad \text { for } \theta \in \Omega$$</p><ul><li>where $g_n$ is the marginal joint p.d.f. or p.f. of $X_1, . . . , X_n$.</li></ul><p>$$g_{n}(\boldsymbol{x})=\int_{\Omega} f_{n}(\boldsymbol{x} \mid \theta) \xi(\theta) d \theta$$</p><h3 id=conjugate-prior-distributions>Conjugate Prior Distributions</h3><h3 id=bayes-estimators>Bayes Estimators</h3><ul><li>A Bayes estimator is an estimator that is chosen to minimize the posterior mean of some measure of how far the estimator is from the parameter, such as squared error or absolute error.</li><li>expected loss：</li></ul><p>$$E[L(\theta, a) \mid \boldsymbol{x}]=\int_{\Omega} L(\theta, a) \xi(\theta \mid \boldsymbol{x}) d \theta$$</p><ul><li>For each possible value $x$ of $X$, let $\delta^\star(\boldsymbol x)$ be a value of a such that $E[L(\theta, a)|x]$ is minimized. Then $\delta^\star$ is called a Bayes estimator of $\theta$.</li><li>Suppose that the squared error loss function is used and that the posterior mean of $\theta, E(\theta \mid \boldsymbol{X}),$ is finite. Then, a Bayes estimator of $\theta$ is $\delta^{*}(\boldsymbol{X})=E(\theta \mid \boldsymbol{X})$.</li><li>To apply the theory, it is necessary to specify a particular loss function, such as the squared error or absolute error function, and also a prior distribution for the parameter. Meaningful specifications may exist, in principle, but it may be very difficult and time-consuming to determine them.</li></ul><h3 id=maximum-likelihood-estimators>Maximum Likelihood Estimators</h3><ul><li>When the joint p.d.f. $f_n(\boldsymbol{x}|\theta)$ of the observations in a random sample is regarded as a function of $\theta$ for given values of $x_1, . . . , x_n$, it is called the likelihood function.</li><li>If we plug the observed values of the data into the conditional p.f. or p.d.f. of the data given the parameter, the result is a function of the parameter alone, which is called the likelihood function.</li><li>Maximum Likelihood Estimator/Estimate. For each possible observed vector $\boldsymbol{x},$ let $\delta(\boldsymbol{x}) \in \Omega$ denote a value of $\theta \in \Omega$ for which the likelihood function $f_{n}(\boldsymbol{x} \mid \theta)$ is a maximum, and let $\hat{\theta}=\delta(\boldsymbol{X})$ be the estimator of $\theta$ defined in this way. The estimator $\hat{\theta}$ is called a maximum likelihood estimator of $\theta .$ After $\boldsymbol{X}=\boldsymbol{x}$ is observed, the value $\delta(\boldsymbol{x})$ is called a maximum likelihood estimate of $\theta$.</li></ul><h4 id=limitations-of-maximum-likelihood-estimation>Limitations of Maximum Likelihood Estimation</h4><p>Nonexistence of an M.L.E.</p><p>$$f(x \mid \theta)=\left{\begin{array}{ll}\frac{1}{\theta} & \text { for } 0&lt;x&lt;\theta \0 & \text { otherwise }\end{array}\right.$$</p><p>Non-uniqueness of an M.L.E.</p><p>$$f_{n}(\boldsymbol{x} \mid \theta)=\left{\begin{array}{ll}1 & \text { for } \theta \leq x_{i} \leq \theta+1,(i=1, \ldots, n) \0 & \text { otherwise }\end{array}\right.$$</p><p>Thus, it is possible to select as an M.L.E. any value of $\theta$ in the interval</p><p>$$ \max \left{x_{1}, \ldots, x_{n}\right}-1 \leq \theta \leq \min \left{x_{1}, \ldots, x_{n}\right} $$</p><p>Sampling from a Mixture of Two Distributions</p><h3 id=properties-of-maximum-likelihood-estimators>Properties of Maximum Likelihood Estimators</h3><ul><li>Invariance: Let $\hat\theta$ be an M.L.E. of $\theta$, and let $g(\theta)$ be a function of $\theta$. Then an M.L.E. of $g(\theta)$ is $g(\hat\theta)$.</li><li>Consistency: the sequence of M.L.E.’s converges in probability to the unknown value of $\theta$ as $n \rightarrow \infty$.</li></ul><h2 id=sampling-distributions-of-estimators>Sampling Distributions of Estimators</h2><h3 id=joint-distribution-of-the-sample-mean-and-sample-variance>Joint Distribution of the Sample Mean and Sample Variance</h3><ul><li>Let $X_{1}, \ldots, X_{n}$ be a random sample from the normal distribution with mean $\mu$ and variance $\sigma^{2}$. Then the sample mean $\hat{\mu}=\bar{X}{n}=\frac{1}{n} \sum{i=1}^{n} X_{i}$ and sample variance $\widehat{\sigma^{2}}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}_{n}\right)^{2}$ are independent random variables. Furthermore, $\hat{\mu}$ has the normal distribution with mean $\mu$ and variance $\sigma^{2} / n,$ and $n \widehat{\sigma}^{2} / \sigma^{2}$ has a chi-square distribution with $n-1$ degrees of freedom.</li></ul><h3 id=confidence-intervals>Confidence Intervals</h3><ul><li>We can find an interval $(A, B)$ that we think has high probability of containing $θ$. The length of such an interval gives us an idea of how closely we can estimate $θ$.</li><li>Let $X = (X_1, . . . , X_n)$ be a random sample from a distribution that depends on a parameter (or parameter vector) $\theta$. Let $g(\theta)$ be a real-valued function of $\theta$. Let $A \leq B$ be two statistics that have the property that for all values of $\theta$,</li></ul><p>$$Pr(A &lt; g(\theta) &lt; B) ≥ \gamma$$</p><ul><li>Then the random interval $(A, B)$ is called a coefficient $\gamma$ confidence interval for $g(θ)$.</li></ul><h3 id=unbiased-estimators>Unbiased Estimators</h3><ul><li>Let $\delta$ be an estimator of a function $g$ of a parameter $\theta$. We say that $\delta$ is unbiased if $E_{\theta}[\delta(X)] = g(\theta)$ for all values of $\theta$.</li><li>The quality of an unbiased estimator must be evaluated in terms of its variance or its M.S.E.</li><li>Limitations:<ul><li>Nonexistence of an Unbiased Estimator<ul><li>suppose that $X_1, . . . , X_n$ form n Bernoulli trials for which the parameter $p$ is unknown. It can be shown that there will be no unbiased estimator of $p^{1/2}$.</li></ul></li><li>Inappropriate Unbiased Estimators<ul><li>Consider an infinite sequence of Bernoulli trials for which the parameter $p$ is unknown $(0 &lt; p &lt; 1)$, and let $X$ denote the number of failures that occur before the first success is obtained.</li></ul></li></ul></li></ul><h2 id=testing-hypotheses>Testing Hypotheses</h2><h3 id=problems-of-testing-hypotheses>Problems of Testing Hypotheses</h3><p>Concepts:</p><ul><li><p>Because of focusing on type I error instead of type II error, we usually regard type I error as having more serious consequences than type II error.</p></li><li><p>Hence, in formulating null and alternative hypotheses, we use the more conservative hypothesis as the null hypothesis, and will only reject it when the data provide statistically significant evidence against it.</p></li><li><p>Type I & II Error</p><p>$$\begin{array}{r|cc} &\text{Actually $H_0$ is True}&\text{Actually $H_1$ is True}\ \hline\text{Do not reject $H_0$}& \text{Correct} &\text{Type II Error} \ \text{Reject $H_0$} & \text{Type I Error} &\text{Correct} \ \end{array}$$</p></li><li><p>Size</p><ul><li>Loosely, the size $\alpha(\delta)$ of a given test $\delta$ is the maximum probability of type I error.</li></ul></li><li><p>Level</p><ul><li>$\delta$ is a level $\alpha_0$ test if and only if $\alpha(\delta)\leq\alpha_0$.</li></ul></li><li><p>$p$-value</p><ul><li>A $p$-value is a probability that provides a measure of the evidence against the null hypothesis provided by the sample.</li><li>In general, the $p$-value is the smallest level $\alpha_0$ such that we would reject the null hypothesis at level $\alpha_0$ with the observed data.</li><li>For each $x$, let $\delta_x$ be the test that rejects $H_0$ if $X\geq x$. Then the $p$-value equals:</li></ul><p>$$ \sup <em>{\theta \in \Omega</em>{0}} \pi\left(\theta | \delta_{x}\right)=\sup <em>{\theta \in \Omega</em>{0}} \operatorname{Pr}(X \geq x | \theta)</p><p>$$</p></li><li><p>Critical Value</p><ul><li>The critical values are the boundaries of the acceptance region of the test.</li><li>The critical value will be a quantile of a certain distribution.</li></ul></li></ul><h3 id=the-t-test>The t Test</h3><h3 id=comparing-the-means-of-two-normal-distributions>Comparing the Means of Two Normal Distributions</h3><h3 id=the-f-distributions>The F Distributions</h3><h2 id=categorical-data-and-nonparametric-methods>Categorical Data and Nonparametric Methods</h2><h3 id=tests-of-goodness-of-fit>Tests of Goodness-of-Fit</h3><h3 id=goodness-of-fit-for-composite-hypotheses>Goodness-of-Fit for Composite Hypotheses</h3><h3 id=contingency-tables>Contingency Tables</h3><h3 id=tests-of-homogeneity>Tests of Homogeneity</h3><h3 id=simpsons-paradox>Simpson’s Paradox</h3><h2 id=four-steps-to-hypothesis-testing>Four Steps to Hypothesis Testing</h2><h3 id=compute-the-test-statistic>Compute the test statistic</h3><table><thead><tr><th style=text-align:left>Situation</th><th>Null Hypothesis</th><th>Test Statistic</th><th>Critical Region</th></tr></thead><tbody><tr><td style=text-align:left>Mean of a Population (known $\sigma$)</td><td>$\mu=\mu_0$</td><td>$U=\frac{\bar x-\mu_0}{\sigma/\sqrt{n}}$</td><td>${\lvert U\rvert\geq U_{1-\alpha/2}}$</td></tr><tr><td style=text-align:left>Mean of a Population (unknown $\sigma$)</td><td>$\mu=\mu_0$</td><td>$T=\frac{\bar x-\mu_0}{s/\sqrt{n}}$</td><td>${\lvert T\rvert\geq t_{1-\alpha/2}(n-1)}$</td></tr><tr><td style=text-align:left>Proportion of a Population</td><td>$p= p_0$</td><td>$U=\frac{\hat{p}-p_{0}}{\sqrt{\frac{p_{0}\left(1-p_{0}\right)}{n}}}$</td><td>${\lvert U\rvert\geq U_{1-\alpha/2}}$</td></tr><tr><td style=text-align:left>Means of Two Populations (known $\sigma$)</td><td>$\mu_1=\mu_2$</td><td>$U=\frac{(\bar x-\bar{y})}{\sqrt{\frac{\sigma_{1}^{2}}{m}+\frac{\sigma_{2}^{2}}{n}}}$</td><td>${\lvert U\rvert\geq U_{1-\alpha/2}}$</td></tr><tr><td style=text-align:left>Means of Two Populations (unknown $\sigma$ and $\sigma_1=\sigma_2$ )</td><td>$\mu_1=\mu_2$</td><td>$T=\frac{\bar x-\bar y}{s_p\sqrt{\frac{1}{m}+\frac{1}{n}}},\ s_{p}^{2}=\frac{(m-1) s_{x}^{2}+(n-1) s_{y}^{2}}{m+n-2}$</td><td>${\lvert T\rvert\geq t_{1-\alpha/2(m+n-2)}}$</td></tr><tr><td style=text-align:left>Means of Two Populations (unknown $\sigma_1,\sigma_2,\sigma_1\neq\sigma_2$ and large sample)</td><td>$\mu_1=\mu_2$</td><td>$U=\frac{\bar x-\bar y}{\sqrt{\frac{s_x^2}{m}+\frac{s_y^2}{n}}}$</td><td>${\lvert U \rvert\geq U_{1-\alpha/2}}$</td></tr><tr><td style=text-align:left>Means of Two Populations (unknown $\sigma_1,\sigma_2,\sigma_1\neq\sigma_2$ and small sample)[Do not need to know]</td><td>$\mu_1=\mu_2$</td><td>$T=\frac{\bar x-\bar y}{\sqrt{\frac{s_x^2}{m}+\frac{s_y^2}{n}}}$</td><td>${\lvert T\rvert\geq t_{1-\alpha/2}(l-1)},\l=\frac{s^{4}}{\frac{s_{x}^{4}}{m^{2}(m-1)}+\frac{s_{y}^{4}}{n^{2}(n-1)}},s_{0}^{2}=\frac{s_{x}^{2}}{m}+\frac{s_{y}^{2}}{n}$</td></tr><tr><td style=text-align:left>Proportion of Two Populations</td><td>$p_1=p_2$</td><td>$U=\frac{\hat{p}<em>{1}-\hat{p}</em>{2}}{\sqrt{\hat{p}(1-\hat{p})\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)}}, \hat{p}=\frac{n_{1} \hat{p}<em>{1}+n</em>{2} \hat{p}<em>{2}}{n</em>{1}+n_{2}}$</td><td>${\lvert U\rvert\geq U_{1-\alpha/2}}$</td></tr><tr><td style=text-align:left>Matched Pairs</td><td>$\delta=\mu_1-\mu_2,\delta=\mu_0$</td><td>$T=\frac{\delta-\mu_0}{s_{\delta}/\sqrt{n}}$</td><td>${\lvert T\rvert \geq t_{1-\alpha/2}(n-1)}$</td></tr><tr><td style=text-align:left>Variance of Two Populations</td><td>$\sigma_1^2=\sigma_2^2$</td><td>$F=\frac{s_x}{s_y}$</td><td>$\left{F \leq F_{\alpha/2}(m-1, n-1) \text { or } F \geq F_{1-\alpha / 2}(m-1, n-1)\right}$</td></tr><tr><td style=text-align:left>Coefficient</td><td>$\hat\beta_j=\beta_j^{\star}$</td><td>$T=\frac{\hat\beta_j-\beta_j^{\star}}{SE(\hat \beta_j)}$</td><td>${\lvert T\rvert \geq t_{1-\alpha/2}(n-k-1)}$</td></tr><tr><td style=text-align:left>Goodness of Fit (known $p_i$)</td><td>$P(A_i)=p_i$</td><td>$Q=\sum_{i=1}^{k} \frac{\left(N_{i}-n p_{i}\right)^{2}}{n p_{i}}$</td><td>${Q\geq \chi^2_{1-\alpha}(k-1)}$</td></tr><tr><td style=text-align:left>Goodness of Fit (unknown $p_i$)</td><td>$P(A_i)=p_i$</td><td>$Q=\sum_{i=1}^{k} \frac{\left[N_{i}-n \pi_{i}(\hat{\theta})\right]^{2}}{n \pi_{i}(\hat{\theta})}$</td><td>${Q\geq \chi^2_{1-\alpha}(k-s-1)}$</td></tr><tr><td style=text-align:left>Independence in Contingency Tables</td><td>$p_i, p_j\ independent$</td><td>$Q=\sum_{i=1}^{R} \sum_{j=1}^{c} \frac{\left(N_{i j}-\hat{E}<em>{i j}\right)^{2}}{\hat{E}</em>{i j}}$</td><td>${Q\geq \chi^2_{1-\alpha}((R-1)(C-1))}$</td></tr></tbody></table><p><strong>Note:</strong> $\bar x,\bar y$ is the sample mean. $s,s_x,s_y$ is the standard deviation of sample. i.e. $s=\sqrt{\frac{\sum\left(X_{i}-\bar{X}\right)^{2}}{n-1}}$.</p></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/Statistics-and-Machine-Learning>Statistics and Machine Learning</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://maeve-l.github.io/js/graph.27e8521c25c27c79dea35f434c486167.js></script>
<script>drawGraph("https://maeve-l.github.io/Statistical-Inference","https://maeve-l.github.io",[{"/moc":"#4388cc"}],-1,!0,!1,!0)</script></div></div><div id=contact_buttons><footer><p>Made by Maeve Liu using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=/>Home</a></li></ul></footer></div><script src=https://maeve-l.github.io/js/popover.e57188d2e4c06b0654e020b3a734bb62.min.js></script>
<script>initPopover("https://maeve-l.github.io")</script></div></body></html>