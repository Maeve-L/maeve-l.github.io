## 简单回归模型

$$
y=\beta_{0}+\beta_{1} x+u
$$

**零条件均值**假定下：
$$
\begin{array}{l}{\mathrm{E}(u)=0} \\ {\operatorname{Cov}(x, u)=\mathrm{E}(x u)=0}\end{array}
$$
矩估计：
$$
\begin{array}{c}{\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)=0} \\ {\frac{1}{n} \sum_{i=1}^{n} x_{i}\left(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i}\right)=0}\end{array}
$$
得到估计的斜率：
$$
\hat{\beta}_{1}=\frac{\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)\left(y_{i}-\overline{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}}
$$
定义总平方和（total sum of squares，SST）、解释平方和（explained sum of squares，SSE）、残差平方和（residual sum of squares，SSR）（也叫做剩余平方和）：
$$
\begin{array}{l}{\mathrm{SST} \equiv \sum_{i=1}^{n}\left(y_{i}-\overline{y}\right)^{2}} \\ {\mathrm{SSE} \equiv \sum_{i=1}^{n}\left(\hat{y}_{i}-\overline{y}\right)^{2}} \\ {\mathrm{SSR} \equiv \sum_{i=1}^{n} \hat u_{i}^{2}}\\ \mathrm{SST}=\mathrm{SSE}+\mathrm{SSR}\end{array}
$$

> 解释平方和有时会被称为回归平方和（regression sum of squares），容易和residual sum of squares混淆。

拟合优度：
$$
R^{2} \equiv \mathrm{SSE} / \mathrm{SST}=1-\mathrm{SSR} / \mathrm{SST}
$$
OLS的相关假定：

- 假定SLR.1: 线性于参数
- 假定SLR.2: 随机抽样
- 假定SLR.3: 解释变量的样本有波动
- 假定SLR.4: 零条件均值
- 假定SLR.5: 同方差性

满足假定1-4，则$\hat\beta_1,\hat\beta_0$估计有无偏性
$$
\begin{aligned} \mathrm{E}\left(\hat{\beta}_{1}\right) &=\beta_{1}+\mathrm{E}\left[\left(1 / \mathrm{SST}_{x}\right) \sum_{i=1}^{n} d_{i} u_{i}\right]=\beta_{1}+\left(1 / \mathrm{SST}_{x}\right) \sum_{i=1}^{n} \mathrm{E}\left(d_i u_{i}\right) \\ &=\beta_{1}+\left(1 / \mathrm{SST}_{x}\right) \sum_{i=1}^{n} d_{i} \mathrm{E}\left(u_{i}\right)=\beta_{1}+\left(1 / \mathrm{SST}_{x}\right) \sum_{i=1}^{n} d_{i} \cdot 0=\beta_{1}\\
\mathrm{E}\left(\hat{\beta}_{0}\right)&=\beta_{0}+\mathrm{E}\left[\left(\beta_{1}-\hat{\beta}_{1}\right) \overline{x}\right]+\mathrm{E}(\overline{u})=\beta_{0}+\mathrm{E}\left[\left(\beta_{1}-\hat{\beta}_{1}\right)\right] \overline{x} =\beta_0\end{aligned}
$$


满足假定1-5，OLS统计量的抽样方差：
$$
\begin{align}
\operatorname{Var}\left(\hat{\beta}_{1}\right)&=\left(1 / \operatorname{SST}_{x}\right)^{2} \operatorname{Var}\left(\sum_{i=1}^{n} d_{i} u_{i}\right)=\left(1 / \operatorname{SST}_{x}\right)^{2}\left(\sum_{i=1}^{n} d_{i}^{2} \operatorname{Var}\left(u_{i}\right)\right)\\&=\left(1 / \mathrm{SST}_{x}\right)^{2}\left(\sum_{i=1}^{n} d_{i}^{2} \sigma^{2}\right)\\&=\sigma^{2}\left(1 / \operatorname{SST}_{x}\right)^{2}\left(\sum_{i=1}^{n} d_{i}^{2}\right)=\sigma^{2}\left(1 / \mathrm{SST}_{x}\right)^{2} \mathrm{SST}_{x}=\sigma^{2} / \mathrm{SST}_{x}\\
\operatorname{Var}\left(\hat{\beta}_{0}\right)&=\frac{\sigma^{2} n^{-1} \sum_{i=1}^{n} x_{i}^{2}}{\sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}}
\end{align}
$$
满足假定1-5，误差方差：
$$
\begin{align}
\hat{\sigma}^{2}=\frac{1}{n-2} \sum_{i=1}^{n} \hat{u}_{i}^{2}=\frac{\mathrm{SSR}}{n-2},\quad E\left(\hat{\sigma}^{2}\right)=\sigma^{2}\end{align}
$$





## 多元回归分析：估计

$$
y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\beta_{3} x_{3}+\cdots+\beta_{k} x_{k}+u
$$



$\hat\beta_i$意为排除了其他变量之后，$x_i$对$y$的影响。

利用现有样本，以$x_i$为因变量，其他变量为自变量做回归，得到残差；再将$y$对OLS残差进行回归就能得到$\hat\beta_i$

[Frisch–Waugh–Lovell theorem的一个简要解释](https://zhuanlan.zhihu.com/p/75716921)

### 基本假定

- 假定MLR.1: 线性于参数
- 假定MLR.2: 随机抽样
- 假定MLR.3: 无完全共线性
- 假定MLR.4: 零条件均值
- 假定MLR.5: 同方差性
- 假定MLR.6: 正态性

### OLS统计量的无偏性

满足假定1-4，OLS估计值就是无偏的。

#### 包含无关变量

在回归模型中包含了无关变量，并不会影响到OLS估计量的无偏性，但会影响其方差。

#### 遗漏变量偏误

$$
\hat{\beta}_{1}=\hat{\beta}_{1}+\hat{\beta}_{2} \hat{\delta}_{1}
$$

其中，$\hat\delta_1$是$x_{i2}$对$x_{i1}$进行简单回归的斜率系数。

由于模型满足假定1-4，所以估计是无偏的，假如遗漏了变量$x_2$：
$$
E\left(\tilde{\beta}_{1}\right)=E\left(\hat{\beta}_{1}+\hat{\beta}_{2} \tilde{\delta}_{1}\right)=E\left(\hat{\beta}_{1}\right)+E\left(\hat{\beta}_{2}\right) \tilde{\delta}_{1}
$$
偏误为：
$$
\operatorname{Bias}\left(\tilde{\beta}_{1}\right)=E\left(\tilde{\beta}_{1}\right)-\beta_{1}=E\left(\hat{\beta}_{1}\right)+E\left(\hat{\beta}_{2}\right) \tilde{\delta}_{1}-\beta_1=\beta_1+\beta_2\tilde\delta_1-\beta_1=\beta_{2} \tilde{\delta}_{1}
$$

### OLS估计量的方差

在假定1-5下：
$$
\operatorname{Var}\left(\beta_{j}\right)=\frac{\sigma^{2}}{\operatorname{SST}_{j}\left(1-R_{j}^{2}\right)}
$$
其中，$\mathrm{SST}_{j}=\sum_{i=1}^{n}\left(x_{i j}-\overline{x}_{j}\right)^{2}$是$x_j$的总样本变异，而$R_j^2$则是将$x_j$对所有其他自变量（并包含一个截距项）进行回归所得到的$R^2$

### 误差项的方差

$$
\sigma^{2}=\frac{\sum_{i=1}^{n} \hat{u}_{i}^{2}}{n-k-1}=\frac{\mathrm{SSR}}{n-k-1}
$$

在假定1-5下，$E\left(\sigma^{2}\right)=\sigma^{2}$.

### 调整后的R方

$$
adj\ R^2=1-\frac{\hat\sigma^2}{\text {SST}/n-1}=1-\frac{(1-R^2)(n-1)}{n-k-1}
$$

### OLS的有效性：高斯马尔可夫定理

在假定1-5下，$\hat{\beta}_{0}, \hat{\beta}_{1}, \cdots, \hat{\beta}_{k}$分别是$\beta_{0}, \beta_{1}, \cdots, \beta_{k}$的最优线性无偏估计量。

最优被定义为最小方差。

### 经典线性模型（CLM）

CLM估计量是最小方差无偏估计。

误差项的正态性导致OLS估计量的正态抽样分布。

## 多元回归分析：推断

### 检验单个参数：t检验

在假定1-6下：
$$
\left(\hat{\beta}_{j}-\beta_{j}\right) / \operatorname{sd}\left(\hat{\beta}_{j}\right) \sim t_{n-k-1}
$$

### 检验多个参数：F检验

$$
F \equiv \frac{\left(\mathrm{SSR}_{r}-\mathrm{SSR}_{u r}\right) / q}{\operatorname{SSR}_{u r} /(n-k-1)}\sim F_{q, n-k-1}
$$

其中，$SSR_r$是受约束模型（变量多）的残差平方和，$SSR_{ur}$是不受约束模型（变量少）的残差平方和，$q$是两个模型的估计参数之差，分母是**不受约束模型**的方差的无偏估计量。

#### 另一种F统计量

$$
F=\frac{\left(R_{ur}^{2}-R_{r}^{2}\right) / q}{\left(1-R_{u r}^{2}\right) /(n-k-1)}=\frac{\left(R_{u r}^{2}-R_{r}^{2}\right) / q}{\left(1-R_{u r}^{2}\right) / d f_{u r}}
$$



## 多元回归分析：OLS的渐近性

不能找到无偏估计量时，找到一个一致估计量也是有用的。

## 异方差性

异方差对估计的无偏性和拟合优度没有影响。

对统计检验有影响。

### OLS估计后的异方差稳健推断

#### 异方差稳健的标准误

$$
\widehat{\operatorname{Var}\left(\hat{\beta}_{j}\right)}=\frac{\sum_{i=1}^{n} \hat{r}_{i j}^{2} \hat{u}_{i}^{2}}{\mathrm{SSR}_{j}^{2}}
$$

其中，$\hat r_{ij}$表示将$x_j$对所有其他自变量做回归所得到的第$i$个残差，而$\text {SSR}_j$则是这个回归的残差平方和。

稳健的$t$统计量的分布在小样本容量的时候可能不是那么接近于$t$分布，从而使我们的推断可能犯错误。

#### 异方差稳健的LM统计量

1. 从约束模型中得到$\tilde u$
2. 将原假设中所排除的每个自变量分别对原假设包含的所有自变量进行回归，如果有$q$个被排除变量，就得到$q$个残差（$\tilde{r}_{1}, \tilde{r}_{2}, \cdots \tilde{r}_{q}$）构成的集合
3. 求出每个$\tilde r_j$和$\tilde u$的积
4. 在不包括截距的情况下将1对$\tilde{r}_{1} \tilde{u}, \tilde{r}_{2} \tilde{u}  \cdots, \tilde{r}_{q} \tilde{u}$做回归，异方差稳健的LM统计量就是$n-\text{SSR}_1$, LM统计量渐近服从$\chi^{2}_q$分布。

### 对异方差的检验

#### 布罗施-帕甘检验

1. 用OLS估计模型，得到残差平方（每次观测得到一个）

2. 用残差平方对所有自变量回归：
   $$
   \hat u^{2}=\delta_{0}+\delta_{1} x_{1}+\delta_{2} x_{2}+\cdots+\delta_{k} x_{k}+v
   $$
   得到这个回归的$R_u^2$

3. 计算F统计量，其服从$F_{k,n-k-1}$分布：
   $$
   F=\frac{R_{u}^{2} / k}{\left(1-R_{u^{2}}^{2}\right) /(n-k-1)}
   $$
   或者LM统计量，渐近服从$\chi^{2}_k$分布：
   $$
   LM=nR_{\hat u^2}^2
   $$
   如果$p$值低于显著性水平，那么就拒绝同方差假设

#### 怀特检验

1. 用OLS估计模型，得到残差和拟合值，计算残差平方和拟合值的平方
2. 做回归$\hat u^2=\delta_0+\delta_1\hat y+\delta_2\hat y^2+v$，得到$R_{\hat u^2}^2$
3. 按上述构造F或LM统计量

### 加权最小二乘估计（WLS）

$$
\begin{aligned} y_{i} / \sqrt{h_{i}}=& \beta_{0} / \sqrt{h_{i}}+\beta_{1}\left(x_{i 1} / \sqrt{h_{i}}\right)+\beta_{2}\left(x_{i 2} / \sqrt{h_{i}}\right)+\cdots+\beta_{k}\left(x_{k} / \sqrt{h_{i}}\right) \\ &+\left(u_{i} / \sqrt{h_{i}}\right) \end{aligned}
$$

### 可行GLS（FGLS）

1. 用OLS估计模型得到残差
2. 将OLS残差平方后取对数得到$log(\hat u^2)$
3. $log(\hat u^2)$对$x_{1}, x_{2}, \cdots, x_{k}$回归，并得到拟合值$\hat g$
4. $\hat{h}=\exp (\hat{g})$
5. 以$1/\hat h$为权，用WLS估计：在上面的方程里用$\hat h_i$取代了$h_i$




## 工具变量回归

### 两阶段最小二乘法回归

- 工具变量和解释变量有关，和干扰项无关
- 先用$Z$对$X$回归，再将估计的$X$对$Y$回归
- TSLS的估计：$\beta^{TSLS}=\frac{cov(Y,Z)}{cov(X,Z)}$

### TSLS的统计推断

- 大样本下TSLS估计是渐进服从正态分布的
$$
\hat\beta_1^{TSLS}\sim N(\beta_1,\sigma^2)
$$
$$
\sigma=\frac{1}{n}\frac{var[(Z_i-\mu_Z)u_i]}{[cov(Z_i,X_i)]^2}=\frac{\sigma^2}{n\sigma_x^2\rho_{x,z}^2}
$$
- $\sigma^2$是干扰项的总体方差，$\rho_{x,z}^2$是解释变量和工具变量的相关系数的平方。
- 求工具变量标准误

### 工具变量有效性的检验

#### 检验相关性

- 先用所有的工具变量和控制变量对$X$做回归
- $F$检验：零假设所有工具变量前面的系数全为零
- $F$统计量大于10，认为是一个弱的工具变量

Anderson-Rubin置信区间

Moreira’s Conditional Likelihood Ratio置信区间

#### 检验外生性

- 只有在过度识别的时候才能使用$J$检验
- 用所有的真实的$X$对$Y$做回归，得到估计的$\hat Y$
- 计算残差
- 用所有的工具变量和控制变量对残差做回归
- $F$检验：零假设是所有工具变量前的系数都为零
- $J=mF$，$m$是工具变量的个数，服从自由度是$m-k$的卡方分布，$k$是解释变量的个数
- $J$检验的零假设是所有的工具变量都是外生的，$J$统计量大说明有工具变量是内生的